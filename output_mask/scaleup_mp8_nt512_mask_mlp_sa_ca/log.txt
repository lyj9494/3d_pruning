2025/10/17 15:02:10 - Accelerator state:
Distributed environment: DistributedType.MULTI_GPU  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16


2025/10/17 15:02:10 - You have chosen to seed([0]) the experiment [scaleup_mp8_nt512_mask_mlp_sa_ca]

2025/10/17 15:02:19 - Loaded [1000] training samples and [1] validation samples

2025/10/17 15:02:19 - Initializing the model...
2025/10/17 15:02:20 - Load pretrained TripoSGDiTModel to initialize TripoSGDiTModel from [/data1/luoyajing/3d_pruning/TripoSG/pretrained_weights/TripoSG]

2025/10/17 15:02:37 - Initializing the optimizer and learning rate scheduler...

2025/10/17 15:02:37 - Number of trainable parameters: 3150

2025/10/17 15:02:37 - Number of grad parameters: 3150

2025/10/17 15:03:01 - Total batch size: [16]
2025/10/17 15:03:01 - Learning rate: [1.0]
2025/10/17 15:03:01 - Gradient Accumulation steps: [4]
2025/10/17 15:03:01 - Total epochs: [1]
2025/10/17 15:03:01 - Total steps: [63]
2025/10/17 15:03:01 - Steps for updating per epoch: [63]
2025/10/17 15:03:01 - Steps for validation: [0]

2025/10/17 15:03:08 - Start training into output_mask/scaleup_mp8_nt512_mask_mlp_sa_ca

2025/10/17 15:04:06 - [000001 / 000063] loss: 1.8991, data_loss: 0.0000, l1_loss: 1.8991, lr: 1.00e+00, ema: 0.0000
2025/10/17 15:05:09 - [000002 / 000063] loss: 0.4592, data_loss: 0.0000, l1_loss: 0.4592, lr: 1.00e+00, ema: 0.4054
2025/10/17 15:06:02 - [000003 / 000063] loss: 0.0999, data_loss: 0.0000, l1_loss: 0.0999, lr: 1.00e+00, ema: 0.5613
2025/10/17 15:06:54 - [000004 / 000063] loss: 0.0442, data_loss: 0.0000, l1_loss: 0.0442, lr: 1.00e+00, ema: 0.6464
2025/10/17 15:07:47 - [000005 / 000063] loss: 0.0422, data_loss: 0.0000, l1_loss: 0.0422, lr: 1.00e+00, ema: 0.7009
2025/10/17 15:08:41 - [000006 / 000063] loss: 0.0422, data_loss: 0.0000, l1_loss: 0.0422, lr: 1.00e+00, ema: 0.7392
2025/10/17 15:09:34 - [000007 / 000063] loss: 0.0422, data_loss: 0.0000, l1_loss: 0.0422, lr: 1.00e+00, ema: 0.7676
2025/10/17 15:10:26 - [000008 / 000063] loss: 0.0422, data_loss: 0.0000, l1_loss: 0.0422, lr: 1.00e+00, ema: 0.7898
2025/10/17 15:11:19 - [000009 / 000063] loss: 0.0422, data_loss: 0.0000, l1_loss: 0.0422, lr: 1.00e+00, ema: 0.8075
